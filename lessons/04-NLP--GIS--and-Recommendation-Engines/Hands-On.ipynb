{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on NLP and Naive Bayes\n",
    "\n",
    "Prepared by Chris Gian for Hack Oregon's Week \n",
    "Sources:\n",
    "- Based mostly on: [Lab 10 of Harvard's CS109](https://github.com/cs109/2015lab10) class. \n",
    "- Adapted for use with sklearn way of text processing: [Working With Text Data](http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html)\n",
    "- Hobson Lane's book: [NLP in Action](https://www.manning.com/books/natural-language-processing-in-action)\n",
    "\n",
    "## Setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# inline plotting\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Vectorize, Transform\n",
    "def make_X(data, min_df_in, stopwords, lowercase,ngram):\n",
    "    vectorizer = CountVectorizer(min_df=min_df_in, stop_words= stopwords, lowercase= lowercase, ngram_range=ngram)\n",
    "    term_doc_matrix = vectorizer.fit_transform(data)\n",
    "    term_doc_matrix = term_doc_matrix.todense()\n",
    "    term_doc_matrix = pd.DataFrame(term_doc_matrix)\n",
    "    transformer = TfidfTransformer()\n",
    "    tfidf = transformer.fit_transform(term_doc_matrix)\n",
    "    tfidf = pd.DataFrame(tfidf.toarray())\n",
    "    return tfidf, vectorizer, transformer\n",
    "\n",
    "# Send scores to a google docs\n",
    "def post_score(name, your_score):\n",
    "    send = 'https://docs.google.com/forms/d/e/1FAIpQLSfOdYRNhf_z3PsHDxMu-IoqaUbUaI9uSHflExgZuBoC1HNvtQ/formResponse?'    \n",
    "    send += 'entry.278237990=' + name\n",
    "    send += '&entry.415269798=' + str(your_score)\n",
    "    r = requests.post(send)\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data + Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read Data\n",
    "critics = pd.read_csv('resources/critics.csv')\n",
    "critics = critics[critics.fresh != 'none']\n",
    "df = critics.copy().dropna()\n",
    "# split\n",
    "X = df.quote\n",
    "y = df.fresh == 'fresh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEeCAYAAABhd9n1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2470d438eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = critics.groupby('fresh').size().plot('bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point you might have employed the following during feature engineering:\n",
    "1. Dealt with contractions and abbreviations\n",
    "2. normalized your vocabulary via\n",
    "    - Case Norming\n",
    "    - Stemming\n",
    "    - Lemmatization\n",
    "3. included a stop words list\n",
    "    - words to exclude such as \"it\" or \"the\" \n",
    "4. Parts of Speech Tagging\n",
    "5. Created \"N-Grams\" where your token looks like \n",
    "    - [\"Ice Cream\", \"New York City\"] rather than\n",
    "    - [New, york, city, ice, cream]\n",
    "6. Dimensionality reduction\n",
    "7. included meta data such as time and critic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Optional Step) Lemmatization and Parts of Speech Tagging using spacy**\n",
    "- Current script will create 5 additional columns\n",
    "- Takes a quite some time for large data frames\n",
    "\n",
    "Need to load lanaguage model [here](https://spacy.io/docs/usage/models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_md #you have to download this from \n",
    "nlp = en_core_web_md.load() \n",
    "data_spacy = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>critic</th>\n",
       "      <th>fresh</th>\n",
       "      <th>imdb</th>\n",
       "      <th>publication</th>\n",
       "      <th>quote</th>\n",
       "      <th>review_date</th>\n",
       "      <th>rtid</th>\n",
       "      <th>title</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tokens_stopwords</th>\n",
       "      <th>lemma</th>\n",
       "      <th>pos</th>\n",
       "      <th>stopword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Derek Adams</td>\n",
       "      <td>fresh</td>\n",
       "      <td>114709</td>\n",
       "      <td>Time Out</td>\n",
       "      <td>So ingenious in concept, design and execution ...</td>\n",
       "      <td>2009-10-04</td>\n",
       "      <td>9559</td>\n",
       "      <td>Toy story</td>\n",
       "      <td>[So, ingenious, in, concept, ,, design, and, e...</td>\n",
       "      <td>[So, ingenious, in, concept, ,, design, and, e...</td>\n",
       "      <td>[so, ingenious, in, concept, ,, design, and, e...</td>\n",
       "      <td>[ADV, ADJ, ADP, NOUN, PUNCT, NOUN, CCONJ, NOUN...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Richard Corliss</td>\n",
       "      <td>fresh</td>\n",
       "      <td>114709</td>\n",
       "      <td>TIME Magazine</td>\n",
       "      <td>The year's most inventive comedy.</td>\n",
       "      <td>2008-08-31</td>\n",
       "      <td>9559</td>\n",
       "      <td>Toy story</td>\n",
       "      <td>[The, year, 's, most, inventive, comedy, .]</td>\n",
       "      <td>[The, year, 's, most, inventive, comedy, .]</td>\n",
       "      <td>[the, year, 's, most, inventive, comedy, .]</td>\n",
       "      <td>[DET, NOUN, PART, ADV, ADJ, NOUN, PUNCT]</td>\n",
       "      <td>[False, False, False, False, False, False, False]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            critic  fresh    imdb    publication  \\\n",
       "1      Derek Adams  fresh  114709       Time Out   \n",
       "2  Richard Corliss  fresh  114709  TIME Magazine   \n",
       "\n",
       "                                               quote review_date  rtid  \\\n",
       "1  So ingenious in concept, design and execution ...  2009-10-04  9559   \n",
       "2                  The year's most inventive comedy.  2008-08-31  9559   \n",
       "\n",
       "       title                                             tokens  \\\n",
       "1  Toy story  [So, ingenious, in, concept, ,, design, and, e...   \n",
       "2  Toy story        [The, year, 's, most, inventive, comedy, .]   \n",
       "\n",
       "                                    tokens_stopwords  \\\n",
       "1  [So, ingenious, in, concept, ,, design, and, e...   \n",
       "2        [The, year, 's, most, inventive, comedy, .]   \n",
       "\n",
       "                                               lemma  \\\n",
       "1  [so, ingenious, in, concept, ,, design, and, e...   \n",
       "2        [the, year, 's, most, inventive, comedy, .]   \n",
       "\n",
       "                                                 pos  \\\n",
       "1  [ADV, ADJ, ADP, NOUN, PUNCT, NOUN, CCONJ, NOUN...   \n",
       "2           [DET, NOUN, PART, ADV, ADJ, NOUN, PUNCT]   \n",
       "\n",
       "                                            stopword  \n",
       "1  [False, False, False, False, False, False, Fal...  \n",
       "2  [False, False, False, False, False, False, False]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed = nlp.pipe(iter(data_spacy['quote']), batch_size=1, n_threads=4)\n",
    "stop = []\n",
    "# Create Features\n",
    "tokens, lemma, parts, stopword = [], [], [], []\n",
    "\n",
    "for parsed_doc in parsed:\n",
    "    tokens.append([n.text for n in parsed_doc])\n",
    "    lemma.append([n.lemma_ for n in parsed_doc])\n",
    "    parts.append([n.pos_ for n in parsed_doc])\n",
    "    stopword.append([n.is_stop for n in parsed_doc])\n",
    "\n",
    "# Assign Parsed into Dataframe\n",
    "data_spacy['tokens'] = tokens     \n",
    "data_spacy['tokens_stopwords'] = data_spacy['tokens'].apply(lambda x: [item for item in x if item not in stop])\n",
    "data_spacy['lemma'] = lemma\n",
    "data_spacy['pos'] = parts\n",
    "data_spacy['stopword'] = stopword\n",
    "\n",
    "# Describe resulting frame\n",
    "data_spacy.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Optional Step)** Stop words removal\n",
    "\n",
    "- Note that you can use \"english\" as an argument. \n",
    "- This will take out common english stopwords.\n",
    "- In addition NLTK has a great english stopwords list too. \n",
    "- Finally, you can always supply your own!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords_list = [] # your work here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Features and Create Train Test Split\n",
    "\n",
    "- Sklearn makes a few things easy for you. Check below arguments I set up. There's more if you explore documentation. You can now:\n",
    "    - Normalize casing\n",
    "    - create ngrams using a range (1,1) for single, (1,2) for one to two grams\n",
    "    - stopwords\n",
    "    - min_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12554, 3234), (12554,), (2216, 3234), (2216,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_in = X # YOUR TURN:  <--- Your engineered features go here.\n",
    "X2, vectorizer_2, transformer_2 = make_X(\n",
    "    x_in,\n",
    "    min_df_in= 10, # Option.\n",
    "    stopwords=stopwords_list, # Option edit from above.\n",
    "    lowercase = False, # Option.\n",
    "    ngram = (1,1) # Option. \n",
    ")\n",
    "# Create Hold data set to judge predictive ability on new data\n",
    "seed = 100\n",
    "X_train, X_test,y_train, y_test = train_test_split(X2,y, test_size=0.15, random_state=seed)\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Optional Step)** Dimension Reduction via \"truncated svd\" also called, \"Latent Semantic Indexing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# svd = TruncatedSVD(n_components=500, n_iter=10, random_state=seed)\n",
    "# X_train = svd.fit_transform(X_train)  \n",
    "# X_test = svd.transform(X_test)  \n",
    "# Note that this must be used as classifier because NB does not take negative values.\n",
    "# clf = LinearDiscriminantAnalysis() great for using with Latent Semantic Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose your classifier\n",
    "- Try other classifiers as well\n",
    "\n",
    "If you're using Naive Bayes. There's a single tuning parameter:\n",
    "\n",
    "**Alpha:** [From Wikipedia](https://en.wikipedia.org/wiki/Naive_Bayes_classifier)\n",
    "\n",
    "\"If a given class and feature value never occur together in the training data, then the frequency-based probability estimate will be zero. This is problematic because it will wipe out all information in the other probabilities when they are multiplied. Therefore, it is often desirable to incorporate a small-sample correction, called pseudocount, in all probability estimates such that no probability is ever set to be exactly zero. This way of regularizing naive Bayes is called Laplace smoothing when the pseudocount is one, and Lidstone smoothing in the general case.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MultinomialNB(alpha=1) # A smoothing parameter. \n",
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Training Data: 0.805162\n",
      "Accuracy on Test Data: 0.734206\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on Training Data: %f\" % clf.score(X_train,y_train))\n",
    "print(\"Accuracy on Test Data: %f\" % clf.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.18501805,  0.20938628],\n",
       "       [ 0.05640794,  0.54918773]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "cm = confusion_matrix(y_pred= y_pred, y_true=y_test)\n",
    "cm / cm.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.77      0.47      0.58       874\n",
      "       True       0.72      0.91      0.81      1342\n",
      "\n",
      "avg / total       0.74      0.73      0.72      2216\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_pred = y_pred, y_true = y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post your score!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline, Accuracy score of: 0.734206\n"
     ]
    }
   ],
   "source": [
    "name = \"Baseline\"\n",
    "result = clf.score(X_test,y_test)\n",
    "print('%s, Accuracy score of: %f' % (name, result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "post_score(name = name, your_score = np.round(result,decimals=3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
